{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `lyra-fr` on SageMaker through Model Packages\n",
    "\n",
    "Developed by <a href=\"https://lighton.ai/\"/>LightOn</a>, `lyra-fr` is the largest language model natively trained in French. `lyra-fr` understands natural language instructions in French, and can perform many tasks in zero-shot or few-shot. If you want to know more about the best ways to prompt large language models, you can have a look at the <a href=\"https://muse-docs.lighton.ai/usecases/english/prompt\">documentation</a>. If you are familiar with the prompting literature, advanced techniques like <a href=\"https://arxiv.org/abs/2201.11903\">Chain of Thought</a> also work with it.\n",
    "\n",
    "Summarizing is as easy as adding *R√©sum√© :* after the relevant text snippet, simply change it to *Mots cl√©s :* to perform keywords extraction instead. The only limit is what you can express in text.\n",
    "\n",
    "For example\n",
    "\n",
    ">Extrais les mots cl√©s de l'article suivant: Le corium est un magma m√©tallique et min√©ral constitu√© d'√©l√©ments fondus du c≈ìur d'un r√©acteur nucl√©aire, puis des min√©raux qu'il peut absorber lors de son trajet. Le terme ¬´ corium ¬ª est un n√©ologisme form√© de core (en anglais, pour le c≈ìur d'un r√©acteur nucl√©aire), suivi du suffixe -ium pr√©sent dans le nom de nombreux √©l√©ments du tableau p√©riodique des √©l√©ments : lithium, calcium, uranium, plutonium, h√©lium, strontium, etc. Initialement constitu√© du combustible nucl√©aire (principalement de l'oxyde d'uranium enrichi), des √©l√©ments de l'assemblage combustible et des divers √©quipements du c≈ìur (barres de contr√¥le, instrumentation) ou de la paroi de la cuve du r√©acteur avec lesquels il entre en contact, il se forme √† tr√®s haute temp√©rature (environ 3 000 ¬∞C, temp√©rature de fusion de l'oxyde d'uranium) quand le c≈ìur n'est plus refroidi, comme lors d'accidents nucl√©aires tels ceux de Three Mile Island, de Tchernobyl, ou de Fukushima.\n",
    ">\n",
    ">Mots-cl√©s : corium, r√©acteur nucl√©aire, fusion du c≈ìur\n",
    "\n",
    "This sample notebook shows you how to deploy `lyra-fr` using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. Before running this notebook, please make sure you got this notebook from the model catalog on SageMaker AWS Management Console.\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**.\n",
    "\n",
    "## Contents:\n",
    "1. [Select model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Visualize output](#D.-Visualize-output)\n",
    "   5. [Delete the endpoint](#E.-Delete-the-endpoint)\n",
    "3. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select to the model package\n",
    "Confirm that you received this notebook from model catalog on SageMaker AWS Management Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for Model Packages (initially only us-east-1 and eu-west-1 is supported)\n",
    "model_package_map = {\n",
    "    \"us-east-1\": \"arn:aws:sagemaker:us-east-1:865070037744:model-package/lyra-fr-a195379e8a323e18ae6e72d9881b1265\",\n",
    "    \"eu-west-1\": \"arn:aws:sagemaker:eu-west-1:985815980388:model-package/lyra-fr-a195379e8a323e18ae6e72d9881b1265\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/lightonai/lightonmuse-sagemaker-sdk.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightonsage as lsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "model_package_arn = model_package_map[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sage.Session()\n",
    "\n",
    "runtime_sm_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lyra-fr\"\n",
    "\n",
    "content_type = \"application/json\"\n",
    "\n",
    "real_time_inference_instance_type = (\n",
    "    \"ml.p4d.24xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=model_name, model_data_download_timeout=3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on the parameters of the endpoint Create, see the <a href=\"https://muse-docs.lighton.ai/api/endpoints/create\">docs</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"text\": \"G√©n√®re une liste d'id√©es d'articles sur l'aquarelle.\\n1. L'aquarelle dans l'histoire\\n2.\", \n",
    "    \"params\": {\n",
    "        \"n_tokens\": 43, \n",
    "        \"seed\": 0\n",
    "    }, \n",
    "    \"endpoint\":\"create\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Add code snippet that shows the payload contents>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=model_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    " \n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{payload['text']} ü§ñ {output['outputs'][0][0]['completions'][0]['output_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen above how to use JSON payloads to invoke an endpoint, now let's use the Python SDK. Using the Python SDK provides a better, more streamlined user experience, and it is recommeneded for experimentation.\n",
    "\n",
    "The Python SDK takes care for you of formatting the input, calling the endpoint, and unpacking the output. There is one class per endpoint: `Create`, `Analyse`, `Select`, `Embed`, `Compare` and `Tokenize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Create for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should describe the task you want to carry out in French, for example in the following: *D√©termine si ces commentaires expriment des avis positifs, n√©gatifs ou mitig√©s.*\n",
    "\n",
    "In addition, providing examples improves the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = lsage.Create(\"lyra-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = examples = f\"\"\"D√©termine si ces commentaires expriment des avis positifs, n√©gatifs ou mitig√©s.\n",
    "----------\n",
    "Commentaire : \\\"D√©sol√© mais je n'ai pas de compliment √† faire √† ce garage. Je me trouvais sur Saint-√âtienne j'√©tais crev√© avec bien sur un clou dans le pneu, l employ√© me dit qu il ne pouvais pas me d√©panner, qu'il fallait que je gonfle mon pneu √† 2.4 je suis redescendu √† Roanne il a fallu que je arr√™te mi-chemin pour gonfler mon pneu. Chercher l'erreur\\\"\n",
    "Ce commentaire exprime un avis n√©gatif.\n",
    "----------\n",
    "Commentaire : \\\"Garage avec un service bien meilleur que les concurrents.√áa aurait pu √™tre parfait, si un l√©ger coup de nettoyage sur les jantes lors du remplacement de mes pneus avait √©t√© effectu√©.Mes jantes √©tait pour le coup plus sales qu'√† l'arriv√©e √† cause du liquide utilis√© pour le montage des pneus qui n'a pas √©t√© essuy√©.Mais apr√®s tout √ßa n'est qu'un d√©tail bonne exp√©rience en g√©n√©ral.Je commande !\\\"\n",
    "Ce commentaire exprime un avis positif.\n",
    "----------\n",
    "Commentaire : \\\"C'est correct mais des employ√©s pas forc√©ment tous tr√®s bien qualifi√©s\\\"\n",
    "Ce commentaire exprime un avis mitig√©.\n",
    "----------\n",
    "Commentaire : \\\"Garantie de batterie contraignante et compliqu√©e, du marketing abusif. Si elle est d√©faillante, on vous la change, mais on vous fait payer la main d'≈ìuvre de remplacement !\\\"\n",
    "Ce commentaire exprime un avis n√©gatif.\n",
    "----------\n",
    "Commentaire : \\\"Bon accueil, heure de rendez-vous respect√© et le prix de la r√©vision bien moins cher que dans un garage de marque de voiture, prochaine r√©vision ou n‚Äôimporte quel autre service je le ferai dans chez Norauto, tr√®s bon garage.\\\"\n",
    "Ce commentaire exprime un avis positif.\n",
    "----------\n",
    "Commentaire: \\\"Excellente enseigne. Personnel comp√©tent et agr√©able. Je mets cette adresse en favori.\\\"\n",
    "Ce commentaire exprime un avis\"\"\"\n",
    "outputs, costs, request_id = creator(few_shot_prompt, stop_words=[\".\", \"\\n\"])\n",
    "print(f\"{few_shot_prompt} ü§ñ {outputs[0]['completions'][0]['output_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Select for review classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = lsage.Select(\"lyra-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"√âtant donn√© un commentaire et la cat√©gorie √† laquelle il appartient, d√©termine la sous-cat√©gorie la plus appropri√©e.\n",
    "\n",
    "Commentaire : \\\"Venu pour faire r√©parer ma roue crev√©e, la conseilli√®re a rapidement constater que le pneu n'est pas r√©parable. Elle m'a propos√© des pneus √©quivalents et pris rdv pour l'apr√®s-midi, car un samedi matin, le centre √©tait satur√©. Lors de l'arriv√©e au rdv, prise en charge rapide et 30 min plus tard, le v√©hicule √©tait pr√™t - Avertissement par SMS. M√™me lors du paiement, l'h√¥te de caisse aura √©t√© agr√©able.Excellent accueil et super boulot\\\"\n",
    "Cat√©gorie :\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, costs, request_id = selector(reference=prompt, candidates=[\"accueil et conseils\", \"prise rendez-vous\"])\n",
    "print(f\"{prompt} ü§ñ {outputs[0]['best']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
